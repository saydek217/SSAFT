{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ba7ad8b-5ee5-449a-898b-ca649c05d056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import csv\n",
    "import os\n",
    "import warnings\n",
    "import numpy\n",
    "import torch\n",
    "import tqdm\n",
    "import yaml\n",
    "from torch.utils import data\n",
    "from nets import nn\n",
    "from nets import nn_transformer_last_layer\n",
    "from utils import util\n",
    "from utils.dataset import Dataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "tb_writer = SummaryWriter() #create the log_file \n",
    "import cv2\n",
    "from torchsummary import summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63a9a24-b3b9-4bb3-8d90-6d8fff49624d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85fe6075-c61c-4b4d-b475-ff70cc50a79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 2772752\n"
     ]
    }
   ],
   "source": [
    "model = nn_transformer_last_layer.SSAFT_n(len(params['names'].values())).cuda()\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters: {total_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "241f9654-ebf9-40f3-89cd-d3fd77a17268",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\custom\\lib\\site-packages\\torchsummary\\torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[1;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[0;32m     68\u001b[0m model\u001b[38;5;241m.\u001b[39mapply(register_hook)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# make a forward pass\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# remove these hooks\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m hooks:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\custom\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\custom\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\Custom_yolo\\code\\nets\\nn_transformer_last_layer.py:393\u001b[0m, in \u001b[0;36mSSAFT.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    392\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet(x)\n\u001b[1;32m--> 393\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfpn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    394\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;28mlist\u001b[39m(x))\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\custom\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\custom\\lib\\site-packages\\torch\\nn\\modules\\module.py:1574\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1572\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[0;32m   1573\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1574\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1577\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\custom\\lib\\site-packages\\torchsummary\\torchsummary.py:19\u001b[0m, in \u001b[0;36msummary.<locals>.register_hook.<locals>.hook\u001b[1;34m(module, input, output)\u001b[0m\n\u001b[0;32m     17\u001b[0m m_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (class_name, module_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     18\u001b[0m summary[m_key] \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m---> 19\u001b[0m summary[m_key][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m())\n\u001b[0;32m     20\u001b[0m summary[m_key][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m batch_size\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "summary(model, (3,256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d371d046",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "def learning_rate(args, initial_lrf=0.01, final_lrf=0.001):\n",
    "    def fn(x):\n",
    "        if x <= 80:\n",
    "            # Phase 1: Initial part towards an lrf of 0.01\n",
    "            return (1 - x / 100) * (1.0 - initial_lrf) + initial_lrf\n",
    "        else:\n",
    "            # Phase 2: Transition from 0.01 to 0.001\n",
    "            start_lr_phase_2 = initial_lrf  # Starting LR for the second phase\n",
    "            end_lr_phase_2 = final_lrf  # Ending LR for the second phase\n",
    "            # Adjust the rate for the second phase\n",
    "            return start_lr_phase_2 + (x - 50) / (args['epochs'] - 50) * (end_lr_phase_2 - start_lr_phase_2)\n",
    "    return fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d02fda76-db6a-4784-9a73-96de200306be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initilization\n",
    "def init_weights(m):\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9290f434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, params, initial_lrf=0.01, final_lrf=0.001):\n",
    "    # Model\n",
    "    model = nn.yolo_v8_n(len(params['names'].values())).cuda()\n",
    "    model.apply(init_weights)\n",
    "\n",
    "    # Optimizer\n",
    "    accumulate = max(round(64 / (args.batch_size * args.world_size)), 1)\n",
    "    params['weight_decay'] *= args.batch_size * args.world_size * accumulate / 64\n",
    "    p = [], [], []\n",
    "    \n",
    "    for v in model.modules():\n",
    "        if hasattr(v, 'bias') and isinstance(v.bias, torch.nn.Parameter):\n",
    "            p[2].append(v.bias)\n",
    "        if isinstance(v, torch.nn.BatchNorm2d):\n",
    "            p[1].append(v.weight)\n",
    "        elif hasattr(v, 'weight') and isinstance(v.weight, torch.nn.Parameter):\n",
    "            p[0].append(v.weight)\n",
    "\n",
    "    optimizer = torch.optim.SGD(p[2], params['lr0'], params['momentum'], nesterov=True)\n",
    "    optimizer.add_param_group({'params': p[0], 'weight_decay': params['weight_decay']})\n",
    "    optimizer.add_param_group({'params': p[1]})\n",
    "    del p\n",
    "\n",
    "    # Scheduler\n",
    "    lr_lambda=learning_rate(args, initial_lrf, final_lrf)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=-1)\n",
    "    \n",
    "    # EMA\n",
    "    ema = util.EMA(model) if args.local_rank == 0 else None\n",
    "\n",
    "    filenames = []\n",
    "            \n",
    "    with open('../data/train.txt') as reader:\n",
    "        for filename in reader.readlines():\n",
    "            filename = filename.rstrip().split('/')[-1]\n",
    "            filenames.append('../data/new_data/train/' + filename)\n",
    "\n",
    "    dataset = Dataset(filenames, args.input_size, params, True)\n",
    "    if args.world_size <= 1:\n",
    "        sampler = None\n",
    "    else:\n",
    "        sampler = data.distributed.DistributedSampler(dataset)\n",
    "\n",
    "    loader = data.DataLoader(dataset, args.batch_size, sampler is None, sampler, num_workers=8, pin_memory=True, collate_fn=Dataset.collate_fn)\n",
    "\n",
    "\n",
    "    if args.world_size > 1:\n",
    "        # DDP mode\n",
    "        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
    "        print(f\"Let's use {torch.cuda.device_count()} GPUs!\")\n",
    "        model = DataParallel(model)\n",
    "\n",
    "    # Start training\n",
    "    best = 0\n",
    "    num_batch = len(loader)\n",
    "    amp_scale = torch.cuda.amp.GradScaler()\n",
    "    criterion = util.ComputeLoss(model, params)\n",
    "    learning_rates = []\n",
    "    num_warmup = max(round(params['warmup_epochs'] * num_batch), 1000)\n",
    "    checkpoint_path = './weights/best.pt'\n",
    "        # Resume training if a checkpoint exists\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        start_epoch = checkpoint['epoch'] + 1  # Start from the next epoch\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        best = checkpoint['best_mAP']\n",
    "        loss_history = checkpoint['loss_history']\n",
    "        learning_rates = checkpoint['learning_rates']\n",
    "     \n",
    "        # Other components you may have saved  \n",
    "        print(f\"Resuming training from epoch {start_epoch}\")\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        \n",
    "    with open('weights/step.csv', 'w') as f:\n",
    "        if args.local_rank == 0:\n",
    "            writer = csv.DictWriter(f, fieldnames=['epoch', 'mAP@50', 'mAP'])\n",
    "            writer.writeheader()\n",
    "        for epoch in range(start_epoch, args.epochs):\n",
    "            model.train()\n",
    "            if args.epochs - epoch == 10:\n",
    "                loader.dataset.mosaic = False\n",
    "\n",
    "            m_loss = util.AverageMeter()\n",
    "            if args.world_size > 1:\n",
    "                sampler.set_epoch(epoch)\n",
    "            p_bar = enumerate(loader)\n",
    "            if args.local_rank == 0:\n",
    "                print(('\\n' + '%10s' * 3) % ('epoch', 'memory', 'loss'))\n",
    "            if args.local_rank == 0:\n",
    "                p_bar = tqdm.tqdm(p_bar, total=num_batch)  # progress bar\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            for i, (samples, targets, _) in p_bar:\n",
    "                x = i + num_batch * epoch  # number of iterations\n",
    "                samples = samples.cuda().float() / 255\n",
    "                targets = targets.cuda()\n",
    "            \n",
    "                # Warmup\n",
    "                \n",
    "                if x <= num_warmup:\n",
    "                    #warmup_factor = x / float(num_warmup)\n",
    "                    xp = [0, num_warmup]\n",
    "                    fp = [1, 64 / (args.batch_size * args.world_size)]\n",
    "                    accumulate = max(1, numpy.interp(x, xp, fp).round())\n",
    "                    for j, y in enumerate(optimizer.param_groups):\n",
    "                        if j == 0:\n",
    "                            fp = [params['warmup_bias_lr'], 0.01 * 0.01]\n",
    "                        else:\n",
    "                            fp = [0.0, 0.01 * 0.01]\n",
    "                        y['lr'] = numpy.interp(x, xp, fp)\n",
    "                        if 'momentum' in y:\n",
    "                            fp = [params['warmup_momentum'], params['momentum']]\n",
    "                            y['momentum'] = numpy.interp(x, xp, fp)\n",
    "\n",
    "                # Forward\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(samples)  # forward\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                m_loss.update(loss.item(), samples.size(0))\n",
    "\n",
    "                loss *= args.batch_size  # loss scaled by batch_size\n",
    "                loss *= args.world_size  # gradient averaged between devices in DDP mode\n",
    "                tb_writer.add_scalar('Training Loss/total', m_loss.avg, global_step=epoch) # log the loss\n",
    "\n",
    "                # Backward\n",
    "                amp_scale.scale(loss).backward()\n",
    "\n",
    "                # Optimize\n",
    "                if x % accumulate == 0:\n",
    "                    amp_scale.unscale_(optimizer)  # unscale gradients\n",
    "                    util.clip_gradients(model)  # clip gradients\n",
    "                    amp_scale.step(optimizer)  # optimizer.step\n",
    "                    amp_scale.update()\n",
    "                    optimizer.zero_grad()\n",
    "                    if ema:\n",
    "                        ema.update(model)\n",
    "\n",
    "                # Log\n",
    "                if args.local_rank == 0:\n",
    "                    memory = f'{torch.cuda.memory_reserved() / 1E9:.3g}G'  # (GB)\n",
    "                    s = ('%10s' * 2 + '%10.4g') % (f'{epoch + 1}/{args.epochs}', memory, m_loss.avg)\n",
    "                    p_bar.set_description(s)\n",
    "\n",
    "                del loss\n",
    "                del outputs\n",
    "\n",
    "            # Scheduler\n",
    "            scheduler.step()\n",
    "\n",
    "            # Store the learning rate for plotting\n",
    "            \n",
    "            current_lr = scheduler.optimizer.param_groups[0]['lr']  # Get the current learning rate\n",
    "            learning_rates.append(current_lr)\n",
    "            tb_writer.add_scalar('Learning rate ', current_lr, global_step=epoch) # log the learning rate \n",
    "            \n",
    "            if args.local_rank == 0:\n",
    "                # mAP\n",
    "                last = test(args, params, ema.ema)\n",
    "                tb_writer.add_scalar('mAP', last[1], global_step=epoch)\n",
    "                tb_writer.add_scalar('mAP@50', last[0], global_step=epoch)\n",
    "                tb_writer.add_scalar('Precision', last[2], global_step=epoch)\n",
    "                tb_writer.add_scalar('recall', last[3], global_step=epoch)\n",
    "                writer.writerow({'mAP': str(f'{last[1]:.3f}'),\n",
    "                                 'epoch': str(epoch + 1).zfill(3),\n",
    "                                 'mAP@50': str(f'{last[0]:.3f}')})\n",
    "                f.flush()\n",
    "\n",
    "                # Update best mAP\n",
    "                if last[1] > best:\n",
    "                    best = last[1]\n",
    "\n",
    "                # Save model\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model': copy.deepcopy(ema.ema).half(),\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': scheduler.state_dict(),\n",
    "                    'best_mAP': best,\n",
    "                    'loss_history': m_loss,\n",
    "                    'learning_rates': learning_rates\n",
    "                }\n",
    "\n",
    "\n",
    "                # Save last, best and delete\n",
    "                torch.save(checkpoint, './weights/last.pt')\n",
    "                if best == last[1]:\n",
    "                    torch.save(checkpoint, './weights/best.pt')\n",
    "                del checkpoint\n",
    "\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8a510bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(args, params, model=None):\n",
    "    filenames = []\n",
    "    \n",
    "    with open('../data/val.txt') as reader:\n",
    "        for filename in reader.readlines():\n",
    "            filename = filename.rstrip().split('/')[-1]\n",
    "            filenames.append('../data/new_data/val/' + filename)\n",
    "\n",
    "    dataset = Dataset(filenames, args.input_size, params, False)\n",
    "    loader = data.DataLoader(dataset, 8, False, num_workers=8,\n",
    "                             pin_memory=True, collate_fn=Dataset.collate_fn)\n",
    "\n",
    "    if model is None:\n",
    "        model = torch.load('./weights/best.pt', map_location='cuda')['model'].float()\n",
    "\n",
    "    model.half()\n",
    "    model.eval()\n",
    "    # Configure\n",
    "    iou_v = torch.linspace(0.5, 0.95, 10).cuda()  # iou vector for mAP@0.5:0.95\n",
    "    n_iou = iou_v.numel()\n",
    "\n",
    "    m_pre = 0.\n",
    "    m_rec = 0.\n",
    "    map50 = 0.\n",
    "    mean_ap = 0.\n",
    "    metrics = []\n",
    "    p_bar = tqdm.tqdm(loader, desc=('%10s' * 3) % ('precision', 'recall', 'mAP'))\n",
    "    for samples, targets, shapes in p_bar:\n",
    "        samples = samples.cuda()\n",
    "        targets = targets.cuda()\n",
    "        samples = samples.half()  # uint8 to fp16/32\n",
    "        samples = samples / 255  # 0 - 255 to 0.0 - 1.0\n",
    "        _, _, height, width = samples.shape  # batch size, channels, height, width\n",
    "\n",
    "        # Inference\n",
    "        outputs = model(samples)\n",
    "        # NMS\n",
    "        targets[:, 2:] *= torch.tensor((width, height, width, height)).cuda()  # to pixels\n",
    "        outputs = util.non_max_suppression(outputs, 0.001, 0.65)\n",
    "\n",
    "        # Metrics\n",
    "        for i, output in enumerate(outputs):\n",
    "            labels = targets[targets[:, 0] == i, 1:]\n",
    "            correct = torch.zeros(output.shape[0], n_iou, dtype=torch.bool).cuda()\n",
    "\n",
    "            if output.shape[0] == 0:\n",
    "                if labels.shape[0]:\n",
    "                    metrics.append((correct, *torch.zeros((3, 0)).cuda()))\n",
    "                continue\n",
    "\n",
    "            detections = output.clone()\n",
    "            util.scale(detections[:, :4], samples[i].shape[1:], shapes[i][0], shapes[i][1])\n",
    "\n",
    "            # Evaluate\n",
    "            if labels.shape[0]:\n",
    "                tbox = labels[:, 1:5].clone()  # target boxes\n",
    "                tbox[:, 0] = labels[:, 1] - labels[:, 3] / 2  # top left x\n",
    "                tbox[:, 1] = labels[:, 2] - labels[:, 4] / 2  # top left y\n",
    "                tbox[:, 2] = labels[:, 1] + labels[:, 3] / 2  # bottom right x\n",
    "                tbox[:, 3] = labels[:, 2] + labels[:, 4] / 2  # bottom right y\n",
    "                util.scale(tbox, samples[i].shape[1:], shapes[i][0], shapes[i][1])\n",
    "\n",
    "                correct = numpy.zeros((detections.shape[0], iou_v.shape[0]))\n",
    "                correct = correct.astype(bool)\n",
    "\n",
    "                t_tensor = torch.cat((labels[:, 0:1], tbox), 1)\n",
    "                iou = util.box_iou(t_tensor[:, 1:], detections[:, :4])\n",
    "                correct_class = t_tensor[:, 0:1] == detections[:, 5]\n",
    "                for j in range(len(iou_v)):\n",
    "                    x = torch.where((iou >= iou_v[j]) & correct_class)\n",
    "                    if x[0].shape[0]:\n",
    "                        matches = torch.cat((torch.stack(x, 1), iou[x[0], x[1]][:, None]), 1)\n",
    "                        matches = matches.cpu().numpy()\n",
    "                        if x[0].shape[0] > 1:\n",
    "                            matches = matches[matches[:, 2].argsort()[::-1]]\n",
    "                            matches = matches[numpy.unique(matches[:, 1], return_index=True)[1]]\n",
    "                            matches = matches[numpy.unique(matches[:, 0], return_index=True)[1]]\n",
    "                        correct[matches[:, 1].astype(int), j] = True\n",
    "                correct = torch.tensor(correct, dtype=torch.bool, device=iou_v.device)\n",
    "            metrics.append((correct, output[:, 4], output[:, 5], labels[:, 0]))\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics = [torch.cat(x, 0).cpu().numpy() for x in zip(*metrics)]  # to numpy\n",
    "    if len(metrics) and metrics[0].any():\n",
    "        tp, fp, m_pre, m_rec, map50, mean_ap = util.compute_ap(*metrics)\n",
    "\n",
    "    # Print results\n",
    "    print('%10.3g' * 3 % (m_pre, m_rec, mean_ap))\n",
    "\n",
    "    # Return results\n",
    "    model.float()  # for training\n",
    "    return map50, mean_ap, m_pre, m_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ab9a4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'input_size': 640,\n",
    "    'batch_size': 16,\n",
    "    'local_rank': 0,  # This might be irrelevant in a non-distributed setup\n",
    "    'epochs': 200,\n",
    "    'train': True,  # Set to False if you don't want to train\n",
    "    'test': False,  # Set to True if you want to test\n",
    "    'world_size': 1 # Assuming a single-process setup\n",
    "}\n",
    "\n",
    "# Adjust for potential distributed computing environments, even though it might not be applicable\n",
    "args['local_rank'] = int(os.getenv('LOCAL_RANK', 0))\n",
    "args['world_size'] = int(os.getenv('WORLD_SIZE', 1))\n",
    "\n",
    "if args['world_size'] > 1:\n",
    "    torch.cuda.set_device(device=args['local_rank'])\n",
    "    torch.distributed.init_process_group(backend='nccl', init_method='env://')\n",
    "\n",
    "if args['local_rank'] == 0:\n",
    "    if not os.path.exists('weights'):\n",
    "        os.makedirs('weights')\n",
    "\n",
    "# Assuming util is a module with these functions. If not, you'll need to define them or adjust accordingly.\n",
    "util.setup_seed()\n",
    "util.setup_multi_processes()\n",
    "\n",
    "# Load parameters from args.yaml\n",
    "with open('./utils/args_indoor_openImages.yaml', 'r') as f:\n",
    "    params = yaml.safe_load(f)\n",
    "args_namespace = argparse.Namespace(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bccd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from epoch 22\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    23/200      9.9G     4.849: 100%|██████████████████████████████████████████████| 3616/3616 [09:04<00:00,  6.64it/s]\n",
      " precision    recall       mAP: 100%|██████████████████████████████████████████████| 1034/1034 [03:40<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0.505     0.391     0.249\n",
      "\n",
      "     epoch    memory      loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    24/200     9.93G      4.87:  19%|████████▊                                      | 679/3616 [01:40<06:06,  8.01it/s]"
     ]
    }
   ],
   "source": [
    "train(args_namespace,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "328c0cd2-0ecb-47d6-b133-04e12998b2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 28 10:37:11 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 536.19                 Driver Version: 536.19       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090      WDDM  | 00000000:01:00.0  On |                  Off |\n",
      "|  0%   29C    P8               6W / 450W |    491MiB / 24564MiB |      1%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      6772    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
      "|    0   N/A  N/A     10228    C+G   ...ell\\Dell Peripheral Manager\\DPM.exe    N/A      |\n",
      "|    0   N/A  N/A     10292    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     10872    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A     10896    C+G   ...2txyewy\\StartMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     12844    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     13636    C+G   ...crosoft\\Edge\\Application\\msedge.exe    N/A      |\n",
      "|    0   N/A  N/A     15356    C+G   ...cal\\Microsoft\\OneDrive\\OneDrive.exe    N/A      |\n",
      "|    0   N/A  N/A     15748    C+G   ...cal\\Microsoft\\OneDrive\\OneDrive.exe    N/A      |\n",
      "|    0   N/A  N/A     16972    C+G   ...__8wekyb3d8bbwe\\WindowsTerminal.exe    N/A      |\n",
      "|    0   N/A  N/A     17736    C+G   ...on\\122.0.2365.92\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     24056    C+G   ...on\\122.0.2365.92\\msedgewebview2.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821fb81a-3205-40d9-9f28-3a8d3ba51abc",
   "metadata": {},
   "source": [
    "### Pytroch to ONNX conversion \n",
    "In the next code we convert the saved best model to ONNX format fot inference with other platforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feb4a3fa-29e2-47be-a615-064c2b6d7a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SalahEddine.Laidoudi\\Desktop\\Custom_yolo\\code\\utils\\util.py:72: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  for i, stride in enumerate(strides):\n",
      "C:\\Users\\SalahEddine.Laidoudi\\miniconda3\\envs\\custom\\lib\\site-packages\\torch\\functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3550.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "util.strip_optimizer('./weights/Best200epochsVanilla.pt')  # strip optimizers\n",
    "model = torch.load('./weights/Best200epochsVanilla.pt', map_location='cuda')['model'].float()\n",
    "model.eval()\n",
    "\n",
    "input_names = [ \"actual_input\" ]\n",
    "output_names = [ \"output\" ]\n",
    "dummy_input = torch.randn(1, 3, 640, 640).to(\"cuda\")\n",
    "\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(model, \n",
    "                  dummy_input,\n",
    "                  \"yolov8Vanilla.onnx\",\n",
    "                  verbose=False,\n",
    "                  input_names=input_names,\n",
    "                  output_names=output_names,\n",
    "                  export_params=True,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57805ae4-2f7d-4ba7-8a96-888d4497f7d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "custom",
   "language": "python",
   "name": "custom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
